\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% URL line breaks in references
\def\UrlBreaks{\do\/\do-}

% todos
\usepackage{xcolor}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
	
\title{Monitoring of Scientific Workflows}

\author{Julian Legler, Marcin Ozimirski, Matthias Seiler
	% <-this % stops a space
	\thanks{This paper was produced by students at TU Berlin in the course ''Master Project: Distributed Systems''}% <-this % stops a space
	%\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
}

% The paper headers
\markboth{PJ DS - Monitoring of Scientific Workflows}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}

\maketitle

\begin{abstract}
	
The emergence of Scientific Workflow Management Systems (SWMS) allowed for speeding up scientific progress in different fields\cite{challengesSW}. The main task of such systems is to represent and manage complex distributed scientific computations. They are designed to handle datasets and examine them in a programmatical way. The software can process given jobs effectively and provide results in a requested form. However, nowadays' scientific calculations can involve hundreds of stages, each integrating several models and data sources created by various groups. Because of their complexity, they require a significant amount of resources to be processed. Such projects are well suited to a cloud environment since it allows for scalability in the event of increased resource demand. In these situations, system monitoring software is essential because it provides a better understanding of how resources are utilized when consecutive tasks are being executed. It can help to decrease the costs by choosing the best suitable provider or adjusting parts of the code for a faster execution time. Unfortunately, the built-in monitoring features offered by the SWMS developers and cloud providers deliver only high-level metrics, such as CPU per task, which is not always enough for a detailed resource usage analysis. Therefore the implementation of software allowing for monitoring on a system process level stays on the shoulders of the end users. 
	
\end{abstract}

\begin{IEEEkeywords}
	Scientific Workflow Management Systems, Monitoring, eBPF
\end{IEEEkeywords}

\section{Introduction}
A scientific workflow management system (SWMS) is a commonly used software solution in the field of computational science. It can execute a series of calculations to examine (big) data sets in a structured and organized manner. Such computations are commonly called workflows consisting of tasks related to one another. Management System is responsible for a transparent orchestration and execution of specified tasks, considering dependencies between them. The software allows for an efficient way to process and extract information from the examined data sets, leading to scientific advances in various fields such as biology, physics, and astronomy\cite{challengesSW}. Yet, such workflows are often resource-intensive and require a distributed, dynamic, and scalable architecture to achieve meaningful results in a reasonable amount of time. The cloud and containerization paradigms offer several advantages when deploying these systems, such as accessibility, flexibility, and scalability, which allow workflows to be deployed and executed dynamically. It is a crucial feature as it is expected from a workflow to change its resource needs throughout the execution, depending on an input data set, the parallelism of computed tasks, and their level of complexity. The available resource pool can scale to adjust the computing resources throughout the workflow execution process. However, depending on the technological solutions offered by cloud services, hardware and software factors can vary significantly from one provider to another, which leads to different processing speeds, resource utilization, and error ratio\cite{compareCloud}. Built-in monitoring features provide information on the amount of time and resources needed for the computation of a particular task or resource utilization on the active virtual machines (containers). Unfortunately, for more intense calculations, like machine learning, data delivered by the built-in tools are often insufficient to provide enough information for deep analysis and detection of bottlenecks or finding opportunities for code optimization. In such scenarios, the more detailed (low-level) metrics available, the better conclusions can be drawn regarding the performance of the current solution, for example, allowing to choose hardware best suited for the currently carried out calculations. Possessing exact information on the number of I/O operations, CPU load, or memory usage per system process allows for a more accurate analysis of executed workflows (or their tasks) and the infrastructure on which the calculations are performed. It can open doors for improvement of the system performance and the code. However, the implementation of solutions capable of gathering such information or extension of the already existing ones rests on the shoulders of end users. This project intends to provide a solution allowing the collection of high- and low-level metrics, independently of underlying cloud technology and an SWMS used for computational science. The following report is structured as follows


\section{Related Work}
% TODO: previous research on SWMS, eBPF
Modern science is often conducted on high-performance computing infrastructures. The complexity of scientific experiments and the underlying infrastructure have grown together, reaching the point where it becomes challenging to execute the workflows in a reliable and error-free way. On the scale at which such projects are carried out it is almost certain that software or infrastructure anomalies will occur, which makes the detection and diagnosis of system failures more challenging than ever before. In this article\cite{captureWorkflow}, the researchers decided to address such issues by creating a prototype system monitoring the following properties at any time during and after the execution of the workflow. The premise of the design is to enable the user to answer the following questions:

\begin{itemize}
  \item What is the status of the workflow?
  \item Did the user make changes to the workflow
before executing it?
  \item Why did it fail at a certain point?
  \item What was the execution context of the
workflow directly before it failed?
  \item How many resources does it consume?
  \item How has each activity in the workflow
performed (in the past)?
\end{itemize}

The ability to answer the mentioned questions can be guaranteed by a system monitoring the general properties of the workflow and its slightest details. Therefore, this type of solution usually consists of various components that collect data at different layers, starting from the general parameters of the workflow through the infrastructure on which it is run, ending with individual processes responsible for performing calculations. In addition, the software must be able to map the relevant processes (which can be executed parallelly and in a distributed manner) to the following machines and tasks and publish them in a readable form. Of course, all this makes sense only when one can assign a chronological order of these processes concerning the time domain. Further, an environment in which such scenarios are executed also plays an important role. Regardless of whether the code is invoked in the operating system context or a container created for the task, it is often characterized by restrictive standards. Such standards fulfil the functions of stability and security and thus may not provide any or only limited opportunities for adjustment. However, without it, it may not be possible to access system values available to the system's kernel. 
Fortunately, one does not have to develop everything from scratch as there are open technologies capable of solving a single or part of the listed conditions. In this thesis\cite{viperProbe}, Mr Levin used Linuxâ€™s extended Berkeley Packet Filter (eBPF)\cite{ebpf} software for a collection of system metrics in the containerized environment and exposure of these to a specified endpoint. In a further step, he used Grafana\cite{grafana}, an observability framework, to present the collected information using programmable dashboards. It is a clever strategy as it allows a collection of data from various sources and then processing and presenting it in the form of histograms and graphs created/adjusted by the user. It also reflects well the current trend for creating monitoring solutions. It is based on a combination of available platforms and software to collect and present data that are relevant to a carried-out project. For such approaches, it is not important what generates or where the data comes from, as long as it is published in a suitable format, which is most often JSON or some derivative of XML.

Lorem \todo{stuff} ipsum

\section{Background}
% TODO: describe technologies we are using

\section{Approach}
% TODO: how we implemented tech mentiones in background

\section{Discussion}
% TODO: results and challenges
\subsection{eBPF Challenges}
\subsection{Nextflow Challenges}

\section{Conclusion}
\includegraphics[width=\linewidth]{images/placeholder.jpg}
\todo{remove placehodler}

\section*{Acknowledgments}
We thank our supervisor SÃ¶ren Becker for his enourmous support and patience. 

% References
\bibliography{bib}
\bibliographystyle{IEEEtran}
	
\end{document}
